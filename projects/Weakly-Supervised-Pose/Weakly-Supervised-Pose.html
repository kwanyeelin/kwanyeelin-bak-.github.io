
<!DOCTYPE html>

<script src="http://www.google.com/jsapi" type="text/javascript"></script>
<script type="text/javascript">google.load("jquery", "1.3.2");</script>

<style type="text/css">
	body {
		font-family: "HelveticaNeue-Light", "Helvetica Neue Light", "Helvetica Neue", Helvetica, Arial, "Lucida Grande", sans-serif;
		font-weight:300;
		font-size:18px;
		margin-left: auto;
		margin-right: auto;
		width: 1100px;
	}

	h1 {
		font-weight:300;
	}

	.disclaimerbox {
		background-color: #eee;
		border: 1px solid #eeeeee;
		border-radius: 10px ;
		-moz-border-radius: 10px ;
		-webkit-border-radius: 10px ;
		padding: 20px;
	}

	video.header-vid {
		height: 140px;
		border: 1px solid black;
		border-radius: 10px ;
		-moz-border-radius: 10px ;
		-webkit-border-radius: 10px ;
	}

	img.header-img {
		height: 140px;
		border: 1px solid black;
		border-radius: 10px ;
		-moz-border-radius: 10px ;
		-webkit-border-radius: 10px ;
	}

	img.rounded {
		border: 1px solid #eeeeee;
		border-radius: 10px ;
		-moz-border-radius: 10px ;
		-webkit-border-radius: 10px ;
	}

	a:link,a:visited
	{
		color: #1367a7;
		text-decoration: none;
	}
	a:hover {
		color: #208799;
	}

	td.dl-link {
		height: 160px;
		text-align: center;
		font-size: 22px;
	}

	.layered-paper-big { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
		box-shadow:
		        0px 0px 1px 1px rgba(0,0,0,0.35), /* The top layer shadow */
		        5px 5px 0 0px #fff, /* The second layer */
		        5px 5px 1px 1px rgba(0,0,0,0.35), /* The second layer shadow */
		        10px 10px 0 0px #fff, /* The third layer */
		        10px 10px 1px 1px rgba(0,0,0,0.35), /* The third layer shadow */
		        15px 15px 0 0px #fff, /* The fourth layer */
		        15px 15px 1px 1px rgba(0,0,0,0.35), /* The fourth layer shadow */
		        20px 20px 0 0px #fff, /* The fifth layer */
		        20px 20px 1px 1px rgba(0,0,0,0.35), /* The fifth layer shadow */
		        25px 25px 0 0px #fff, /* The fifth layer */
		        25px 25px 1px 1px rgba(0,0,0,0.35); /* The fifth layer shadow */
		margin-left: 10px;
		margin-right: 45px;
	}


	.layered-paper { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
		box-shadow:
		        0px 0px 1px 1px rgba(0,0,0,0.35), /* The top layer shadow */
		        5px 5px 0 0px #fff, /* The second layer */
		        5px 5px 1px 1px rgba(0,0,0,0.35), /* The second layer shadow */
		        10px 10px 0 0px #fff, /* The third layer */
		        10px 10px 1px 1px rgba(0,0,0,0.35); /* The third layer shadow */
		margin-top: 5px;
		margin-left: 10px;
		margin-right: 30px;
		margin-bottom: 5px;
	}

	.vert-cent {
		position: relative;
	    top: 50%;
	    transform: translateY(-50%);
	}

	hr
	{
		border: 0;
		height: 1.5px;
		background-image: linear-gradient(to right, rgba(0, 0, 0, 0), rgba(0, 0, 0, 0.75), rgba(0, 0, 0, 0));
	}
</style>

<html>
  <head>
		<title>Weakly-Supervised Discovery of Geometry-Aware Representation for 3D Human Pose Estimation</title>
  </head>

  <body>
    <br>
    <center>
    <span style="font-size:30px">Weakly-Supervised Discovery of Geometry-Aware Representation for 3D Human Pose Estimation</span>
	</center>

	<br>


  	<table align=center width=900px>
  	 <tr>
		<td align=center width=80px>
		<center>
		<span style="font-size:18px"><a href="https://kwanyeelin.github.io/">Kwan-Yee Lin*</a></span>
		</center>
		</td>

		<td align=center width=80px>
		<center>
		<span style="font-size:18px"> Xipeng Chen*</span>
		</center>
		</td>

		<td align=center width=80px>
		<center>
		<span style="font-size:18px"> Wentao Liu</span>
		</center>
		</td>

		<td align=center width=80px>
		<center>
		<span style="font-size:18px"><a href="https://scholar.google.com/citations?user=AerkT0YAAAAJ&hl=en">Chen Qian</a></span>
		</center>
		</td>

		<td align=center width=80px>
		<center>
		<span style="font-size:18px"><a href="http://www.linliang.net/">Liang Lin</a></span>
		</center>
		</td>

	 </tr>
	</table>

	<br>
	
	<table align=center width=900px>
  	 <tr>
		<td align=center width=80px>
		<center>
		<span style="font-size:20px">Peking University</span>
		</center>
		</td>

		<td align=center width=80px>
		<center>
		<span style="font-size:20px">Sun Yat-Sen University</span>
		</center>
		</td>

		<td align=center width=80px>
		<center>
		<span style="font-size:20px">SenseTime Research</span>
		</center>
		</td>

	 </tr>
	</table>

	<br>



     
  		  <br>
  		  <table align=center width=900px>
  			  <tr>
  	              <td width=900px>
  					<center>
  	                	<a href="./support/TGaGa.png"><img src = "./support/TGaGa.png" height="600px"></img></href></a><br>
					</center>
  	              </td>
  	          </tr>
  		  </table>

      	  <br>
      	  <p style="text-align:justify">
				Recent studies have shown remarkable advances in 3D human pose estimation from monocular images, with the help of large-scale in-door 3D datasets and sophisticated network architectures. However, the generalizability to different environments remains an elusive goal. In this work, we propose a geometry-aware 3D representation for the human pose to address this limitation by using multiple views in a simple auto-encoder model at the training stage and only 2D keypoint information as supervision. A view synthesis framework is proposed to learn the shared 3D representation between viewpoints with synthesizing the human pose from one viewpoint to the other one. Instead of performing a direct transfer in the raw image-level, we propose a skeleton-based encoder-decoder mechanism to distil only pose-related representation in the latent space. A learning-based representation consistency constraint is further introduced to facilitate the robustness of latent 3D representation. Since the learnt representation encodes 3D geometry information, mapping it to 3D pose will be much easier than conventional frameworks that use an image or 2D coordinates as the input of 3D pose estimator. We demonstrate our approach on the task of 3D human pose estimation. Comprehensive experiments on three popular benchmarks show that our model can significantly improve the performance of state-of-the-art methods with simply injecting the representation as a robust 3D prior.
      	  </p>
  		  <br><br>
		  <hr>
		 <!-- <table align=center width=550px> -->
  		  <table align=center width=1100>
	 		<center><h1>Paper</h1></center>
  			  <tr>
  	              <!--<td width=300px align=left>-->
  	              <!-- <a href="http://arxiv.org/pdf/1603.08511.pdf"> -->
				  <!-- <td><a href="#"><img class="layered-paper-big" style="height:175px" src="./resources/images/paper.png"/></a></td> -->
				  <td><a href="https://arxiv.org/pdf/1903.08839.pdf"><img style="height:180px" src="./support/paper.png"/></a></td>
				  <td><span style="font-size:14pt">Weakly-Supervised Discovery of Geometry-Aware Representation for 3D Human Pose Estimation<br><br>
                          <i>Xipeng Chen, Kwan-Yee Lin, Wentao Liu, Chen Qian, Liang Lin</i><br><br>
				  To Appear in Computer Vision and Pattern Recognition, CVPR 2019. <br>
				  </td>
  	              </td>
              </tr>
  		  </table>
		  <br>

		  <table align=center width=400px>
			  <tr>
				  <td><span style="font-size:14pt"><center>
				  	<a href="https://arxiv.org/pdf/1903.08839.pdf">[PDF]</a>
  	              </center></td>

				  <td><span style="font-size:14pt"><center>
				  	<a href="./support/weakly-supervised-pose-supp.pdf">[Appendix]</a>
  	              </center></td>

				  <td><span style="font-size:14pt"><center>
				  	<a href="./support/weakly-supervised-pose.pptx">[Poster]</a>
  	              </center></td>

				  <td><span style="font-size:14pt"><center>
				  	<a href="./support/weakly-supervised-pose_bibtex.txt">[Bibtex]</a>
  	              </center></td>
              </tr>
  		  </table>
		  	<br>

  		  	<hr>
<!-- 
  		  <table align=center width=1100>
	 		<center><h1>Data (coming soon)</h1></center>
	 		<center>
		   	<div>
				<img src="./support/dog.png" height="170" width=170>
				<img src="./support/cat.jpg" height="170" width=170>
				<img src="./support/horse.jpg" height="170" width=170>
				<img src="./support/giraffe.jpg" height="170" width=170>
				<img src="./support/car.jpg" height="170" width=170>
				<img src="./support/bus.jpg" height="170" width=170>
		 	</div>
		 	</center>
			<br>
			<p class="text-justify">
				We collect a large unpaired image-to-image translation dataset of 10 different categories with large geometry variations, i.e., human-face, cat-face, dog-face, horse, giraffe, cow, lion, bird, bus and car. In average, each dataset has 4500 images for training and 500 images for testing. Each image is center cropped and resized to the same resolution as 256*256.
			</p>
		   	<div>
				<center><a href="" class="btn btn-outline-secondary">[Download]</a></center>
		   	</div>
  		  </table>
		  <br> -->

  		  <!-- <hr> -->



<!--   		  <table align=center width=1100px>
  			  <tr>
  	              <td>
  					<left>
	  		  <center><h1>Acknowledgements</h1></center>
	  		  We would like to thank Kwan-Yee Lin and Jingtan Piao for insightful discussion and their exceptional support.</a>
			</left>
		</td>
		</tr>
		</table> -->

		<br><br>
</body>
</html>
