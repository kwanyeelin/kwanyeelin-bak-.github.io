
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<title>Hallucinated-IQA: No-reference Image Quality Assessment via Adversarial Learning</title>

<!-- Meta tags for search engines to crawl -->
<meta name="robots" content="index,follow">
<meta name="description" content="No-reference image quality assessment (NR-IQA) is a fundamental yet challenging task in low-level computer vision community. The difficulty is particularly pronounced for the limited information, for which the corresponding reference for comparison is typically absent. Although various feature extraction mechanisms have been leveraged from natural scene statistics to deep neural networks in previous methods, the performance bottleneck still exists.
In this work, we propose a hallucination-guided quality regression network to address the issue. We firstly generate a hallucinated reference constrained on the distorted image, to compensate the absence of the true reference. Then, we pair the information of hallucinated reference with the distorted image, and forward them to the regressor to learn the perceptual discrepancy with the guidance of an implicit ranking relationship within the generator, and therefore produce the precise quality prediction. To demonstrate the effectiveness of our approach, comprehensive experiments are evaluated on four popular image quality assessment benchmarks. Our method significantly outperforms all the previous state-of-the-art methods by large margins.">
<meta name="keywords" content="IQA; image quality assessment; Hallucinated-IQA; deep learning; Convolutional network; computer vision;">
<link rel="author" href="http://kwanyeelin.github.io">

<!-- Fonts and stuff -->
<link href="./support/css" rel="stylesheet" type="text/css">
<link rel="stylesheet" type="text/css" href="./support/project.css" media="screen">
<link rel="stylesheet" type="text/css" media="screen" href="./support/iconize.css">
<script async="" src="./support/prettify.js"></script>


</head>


<body>
  <div id="content">
    <div id="content-inner">
      
      <div class="section head">
	<h1>Hallucinated-IQA: No-Reference Image Quality Assessment via Adversarial Learning</h1>

	<div class="authors">
	  <a href="http://kwanyeelin.github.io">Kwan-Yee Lin</a><sup>1</sup>&nbsp;&nbsp;
	  <a href="">Guanxiang Wang</a><sup>2</sup>&nbsp;&nbsp;
	</div>

	<div class="affiliations">
	  <sup>1</sup><a href="">Department of Information Science, School of Mathematical Sciences, Peking University<br></a>
	  <sup>2</sup><a href="">Department of Mathematics, School of Mathematical Sciences, Peking University</a>
	</div>

	  </div>

      
      <center><img src="./support/index.png" border="0" width="85%"></center>
      <div class="section abstract">
	<h2>Abstract</h2>
	<p>
No-reference image quality assessment (NR-IQA) is a fundamental yet challenging task in low-level computer vision community. The difficulty is particularly pronounced for the limited information, for which the corresponding reference for comparison is typically absent. Although various feature extraction mechanisms have been leveraged from natural scene statistics to deep neural networks in previous methods, the performance bottleneck still exists.
In this work, we propose a hallucination-guided quality regression network to address the issue. We firstly generate a hallucinated reference constrained on the distorted image, to compensate the absence of the true reference. Then, we pair the information of hallucinated reference with the distorted image, and forward them to the regressor to learn the perceptual discrepancy with the guidance of an implicit ranking relationship within the generator, and therefore produce the precise quality prediction. To demonstrate the effectiveness of our approach, comprehensive experiments are evaluated on four popular image quality assessment benchmarks. Our method significantly outperforms all the previous state-of-the-art methods by large margins.
	</p>
      </div>
<div class="section downloads">
<!-- 	<h2>Demo</h2><center>
      	<iframe width="420" height="315" src="https://www.youtube.com/embed/IMPjPQSb9g8" frameborder="0" allowfullscreen></iframe>
      </div></center>
      <div class="section downloads"> -->
	<h2>Downloads</h2>
	<center>
	  <ul>
        <li class="grid">
	      <div class="griditem">
		<a href="https://arxiv.org/abs/1804.01681" target="_blank" class="imageLink"><img src="./support/paper.png"></a><br><a href="https://arxiv.org/abs/1804.01681">Paper</a>
		</div>
	      </li>
        <li class="grid">
	      <div class="griditem">
		<a href="./support/sup.pdf" target="_blank" class="imageLink"><img src="./support/sup.png"></a><br><a href="./support/sup.pdf">Supplementary Material</a>
		</div>
	      </li>
	    <li class="grid">
	      <div class="griditem">
		<a href="https://github.com/kwanyeelin/HIQA" target="_blank" class="imageLink"><img src="./support/code.png"></a><br><a href="https://github.com/kwanyeelin/HIQA" target="_blank">Code and Model</a>
		</div>
	      </li>
	    </ul>
	    </center>
	    </div>
	    


<br>
 <div class="section list">
	<h2>Citation</h2>
	
	<div class="section bibtex">
	  <pre>@inproceedings{kwanyee2018hallucinated,
 author = {Lin, Kwan-Yee and Wang, Guangxiang},
 title = {Hallucinated-IQA: No-reference Image Quality Assessment via Adversarial Learning},
 booktitle = {CVPR},
 month = June,
 year = {2018}
} 
	</pre>
	  </div>
      </div>

     <div class="section contact">
	<h2>Contact</h2>
	Kwan-Yee Lin<br><a href="mailto:linjunyi9335@gmail.com">linjunyi9335@gmail.com</a>
      </div>
    </div>
  </div>

</body></html>
